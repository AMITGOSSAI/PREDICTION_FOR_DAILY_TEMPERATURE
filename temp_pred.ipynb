{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vumpnfCnPS-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import  MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g9WtreDNV93"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM, Conv2D, MaxPooling2D, Flatten, Dense\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyZfaqWRQAiU"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('/content/merged_file_data.csv', parse_dates=[0], dayfirst=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyxUV0zDQGcV"
      },
      "outputs": [],
      "source": [
        "# Normalize the features to the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data.iloc[:, 1:])  # Assuming the first column is the date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLY2uLVsQJja"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "scaled_data = pd.DataFrame(scaled_data, columns=data.columns[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hILF0fuTQMxh"
      },
      "outputs": [],
      "source": [
        "# Add the date back\n",
        "scaled_data['Date'] = data.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FksIiT5dQOzP"
      },
      "outputs": [],
      "source": [
        "# Set the date as index\n",
        "scaled_data.set_index('Date', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XaiONhOQR72"
      },
      "outputs": [],
      "source": [
        "# Create sequences of data\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data.iloc[i:i+seq_length, :-1].values)\n",
        "        y.append(data.iloc[i+seq_length, -1])  # Assuming the last column is temperature\n",
        "    return np.array(X), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IT5rVZ5QUia"
      },
      "outputs": [],
      "source": [
        "seq_length = 10  # For example, using 10 days of data to predict the next day's temperature\n",
        "X, y = create_sequences(scaled_data, seq_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W67ZqRmQQXS3"
      },
      "outputs": [],
      "source": [
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5rLmNTWQeU9"
      },
      "outputs": [],
      "source": [
        "# Reshape input data to be compatible with TimeDistributed layer\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY9EYdTDQhhV"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model_cnn_lstm = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P08eSOKeQkWe"
      },
      "outputs": [],
      "source": [
        "# TimeDistributed Conv1D layer\n",
        "model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train.shape[2], X_train.shape[3])))\n",
        "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "model_cnn_lstm.add(TimeDistributed(Flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UimmGj-QQqsK"
      },
      "outputs": [],
      "source": [
        "# LSTM layer\n",
        "model_cnn_lstm.add(LSTM(50, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oILKznFuQxG6"
      },
      "outputs": [],
      "source": [
        "# Fully connected layers\n",
        "model_cnn_lstm.add(Dense(100, activation='relu'))  # First fully connected layer\n",
        "model_cnn_lstm.add(Dropout(0.2))  # Dropout layer for regularization\n",
        "model_cnn_lstm.add(Dense(50, activation='relu'))   # Second fully connected layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJJNC3obQzuR"
      },
      "outputs": [],
      "source": [
        "# Output layer\n",
        "model_cnn_lstm.add(Dense(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNeXuwMJQ1zw"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_cnn_lstm.compile(loss='mse', optimizer=Adam())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knrpnib_Q4RT",
        "outputId": "9cde0740-0b62-4258-b48e-ae40f0138c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed (TimeDist  (None, None, 9, 64)       128       \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, None, 4, 64)       0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, None, 256)         0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50)                61400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               5100      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 71729 (280.19 KB)\n",
            "Trainable params: 71729 (280.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Summary of the model\n",
        "model_cnn_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igtr4MFzRLMK",
        "outputId": "f9646171-3879-4a05-aee9-c3fa6a2da9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "307/307 [==============================] - 9s 22ms/step - loss: 0.0277 - val_loss: 0.0151\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0150 - val_loss: 0.0132\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 7s 22ms/step - loss: 0.0132 - val_loss: 0.0117\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0125 - val_loss: 0.0116\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 6s 18ms/step - loss: 0.0121 - val_loss: 0.0114\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 6s 19ms/step - loss: 0.0118 - val_loss: 0.0117\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0116 - val_loss: 0.0109\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 7s 23ms/step - loss: 0.0111 - val_loss: 0.0102\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0110 - val_loss: 0.0119\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 6s 18ms/step - loss: 0.0108 - val_loss: 0.0099\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model_cnn_lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewbQvMcTRPCY",
        "outputId": "4217e400-6434-4bf9-cb84-77f4957cb504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss (MSE): 0.00970414187759161\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss = model_cnn_lstm.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss (MSE): {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvKx0V3hRSD1",
        "outputId": "155eed16-2455-4def-ef29-9a89cb27d8a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 1s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "y_pred = model_cnn_lstm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUgm1_VhRUYx"
      },
      "outputs": [],
      "source": [
        "# Inverse transform the predictions if needed\n",
        "y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIsDPeOlRXKY"
      },
      "outputs": [],
      "source": [
        "# Function to calculate RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e3p210ehf9C",
        "outputId": "58972ad5-fbbc-4ccc-e761-1ea295c2e2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 3.2187160931547667\n"
          ]
        }
      ],
      "source": [
        "# prompt: print rmse value\n",
        "\n",
        "print(f'RMSE: {rmse(y_test, y_pred)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gddWqHkRZKR"
      },
      "outputs": [],
      "source": [
        "# Function to calculate Correlation Coefficient (CC)\n",
        "def correlation_coefficient(y_true, y_pred):\n",
        "    return np.corrcoef(y_true, y_pred)[0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3-DcA5uSOXm",
        "outputId": "2444971e-71a1-4235-8e9c-844e3071a8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0111\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 7s 21ms/step - loss: 0.0106\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0104\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0101\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 6s 18ms/step - loss: 0.0102\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0097\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 6s 20ms/step - loss: 0.0098\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0095\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0093\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 6s 21ms/step - loss: 0.0074\n",
            "77/77 [==============================] - 0s 5ms/step\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 7s 21ms/step - loss: 0.0053\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0049\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0046\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 7s 21ms/step - loss: 0.0045\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0042\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0042\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 6s 20ms/step - loss: 0.0041\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0042\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 6s 18ms/step - loss: 0.0041\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 5s 17ms/step - loss: 0.0039\n",
            "77/77 [==============================] - 0s 5ms/step\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 6s 21ms/step - loss: 0.0040\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0040\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0039\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 6s 19ms/step - loss: 0.0039\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 8s 25ms/step - loss: 0.0039\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 6s 19ms/step - loss: 0.0039\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0038\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 6s 19ms/step - loss: 0.0038\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 5s 16ms/step - loss: 0.0037\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 7s 23ms/step - loss: 0.0037\n",
            "77/77 [==============================] - 1s 9ms/step\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0038\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0037\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 6s 21ms/step - loss: 0.0037\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0038\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 0.0037\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 6s 20ms/step - loss: 0.0036\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 5s 18ms/step - loss: 0.0036\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 6s 20ms/step - loss: 0.0035\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0035\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0036\n",
            "77/77 [==============================] - 0s 5ms/step\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0034\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 6s 21ms/step - loss: 0.0034\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0034\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 5s 15ms/step - loss: 0.0034\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 6s 19ms/step - loss: 0.0033\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 4s 14ms/step - loss: 0.0034\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 6s 18ms/step - loss: 0.0033\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 5s 17ms/step - loss: 0.0033\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 6s 20ms/step - loss: 0.0033\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 6s 21ms/step - loss: 0.0032\n",
            "77/77 [==============================] - 0s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "# Define the number of folds\n",
        "n_folds = 5\n",
        "\n",
        "# Initialize the KFold object\n",
        "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store the results\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Iterate through each fold\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    # Get the training and testing data for the current fold\n",
        "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # Reshape the training and testing data\n",
        "    X_train_fold = X_train_fold.reshape((X_train_fold.shape[0], X_train_fold.shape[1], X_train_fold.shape[2], 1))\n",
        "    X_test_fold = X_test_fold.reshape((X_test_fold.shape[0], X_test_fold.shape[1], X_test_fold.shape[2], 1))\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    model_cnn_lstm.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=1)\n",
        "    # Evaluate the model on the current fold\n",
        "    loss = model_cnn_lstm.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
        "    loss_per_fold.append(loss)\n",
        "\n",
        "    # Calculate the accuracy on the current fold\n",
        "    y_pred = model_cnn_lstm.predict(X_test_fold)\n",
        "    y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "    y_test_fold = scaler.inverse_transform(np.concatenate((np.zeros((y_test_fold.shape[0], scaled_data.shape[1] - 1)), y_test_fold.reshape(-1, 1)), axis=1))[:, -1]\n",
        "    accuracy = correlation_coefficient(y_test_fold, y_pred)\n",
        "    acc_per_fold.append(accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip6bo28WWMWI",
        "outputId": "e9dfb2fc-55e2-4003-fe99-f107b8b27152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy across folds: 0.935861157141316\n",
            "Average loss across folds: 0.003856461029499769\n",
            "RMSE: 1.928430226099921\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average accuracy across all folds\n",
        "average_acc = np.mean(acc_per_fold)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average accuracy across folds:\", average_acc)\n",
        "\n",
        "# Calculate the average loss across all folds\n",
        "average_loss = np.mean(loss_per_fold)\n",
        "\n",
        "# Print the average loss\n",
        "print(\"Average loss across folds:\", average_loss)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_values = rmse(y_test_fold, y_pred)\n",
        "print(f'RMSE: {rmse_values}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvDGA3hAQl7",
        "outputId": "8d243592-34e1-4a11-fc4d-7e3039419827"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3067,)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpGiXCFJRcp2",
        "outputId": "d64dc8b8-7b6e-4cb8-cf04-cf5da249a5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation Coefficient: 0.9458887292307925\n"
          ]
        }
      ],
      "source": [
        "# Calculate Correlation Coefficient (CC)\n",
        "cc_value = correlation_coefficient(y_test_fold, y_pred)\n",
        "print(f'Correlation Coefficient: {cc_value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HlN0mOSAWuU",
        "outputId": "21de2bed-8770-4f03-fe82-bb2fa68be28f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12264,)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REJmvdkaRfDs",
        "outputId": "ed1ef8c1-5dce-40cb-c1d7-46b0b084bf62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [33.92348544 33.11139657 37.43157912 29.79710939 30.10754958 33.76706372\n",
            " 20.78847024 33.443801   32.20172865 34.09902022]\n",
            "Actual values: [32.82686615 39.18675613 37.745876   34.26612473 33.67470932 32.7926445\n",
            " 42.33344269 34.84613419 27.58231354 21.32916641]\n"
          ]
        }
      ],
      "source": [
        "# Compare predictions with actual values\n",
        "print(f'Predictions: {y_pred[:10]}')\n",
        "print(f'Actual values: {y_test[:10]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFT71i6eteQA",
        "outputId": "09ff172e-8c1f-4375-aed7-46bcf4113e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 64)                18944     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19009 (74.25 KB)\n",
            "Trainable params: 19009 (74.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 5s 9ms/step - loss: 0.0170 - val_loss: 0.0057\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 2s 7ms/step - loss: 0.0059 - val_loss: 0.0045\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 2s 7ms/step - loss: 0.0051 - val_loss: 0.0043\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 3s 9ms/step - loss: 0.0048 - val_loss: 0.0042\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 3s 10ms/step - loss: 0.0047 - val_loss: 0.0040\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 2s 7ms/step - loss: 0.0045 - val_loss: 0.0039\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 2s 7ms/step - loss: 0.0044 - val_loss: 0.0048\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 2s 7ms/step - loss: 0.0043 - val_loss: 0.0037\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 2s 8ms/step - loss: 0.0043 - val_loss: 0.0038\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 4s 11ms/step - loss: 0.0042 - val_loss: 0.0037\n",
            "Test Loss (MSE): 0.00408964604139328\n",
            "96/96 [==============================] - 1s 3ms/step\n",
            "RMSE: 2.0895221220687654\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv('/content/merged_file_data.csv', parse_dates=[0], dayfirst=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data.iloc[:, 1:])\n",
        "scaled_data = pd.DataFrame(scaled_data, columns=data.columns[1:])\n",
        "scaled_data['Date'] = data.iloc[:, 0]\n",
        "scaled_data.set_index('Date', inplace=True)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data.iloc[i:i+seq_length, :-1].values)\n",
        "        y.append(data.iloc[i+seq_length, -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1)\n",
        "])\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "model_lstm.summary()\n",
        "\n",
        "# Train the LSTM model\n",
        "model_lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate and predict with the LSTM model\n",
        "loss = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss (MSE): {loss}')\n",
        "y_pred = model_lstm.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f'RMSE: {rmse(y_test, y_pred)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tCraM33ttfv",
        "outputId": "5a6aa7df-0bb3-4ec3-acca-479826c59d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 8, 7, 32)          320       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 4, 3, 32)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 64)                24640     \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25025 (97.75 KB)\n",
            "Trainable params: 25025 (97.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 2s 5ms/step - loss: 0.0148 - val_loss: 0.0055\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0061 - val_loss: 0.0046\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0042\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0053\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 1s 3ms/step - loss: 0.0045 - val_loss: 0.0039\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0038\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 2s 5ms/step - loss: 0.0045 - val_loss: 0.0040\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 2s 6ms/step - loss: 0.0043 - val_loss: 0.0050\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 1s 5ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Test Loss (MSE): 0.004541432484984398\n",
            "96/96 [==============================] - 0s 2ms/step\n",
            "RMSE: 2.2019142878942084\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv('/content/merged_file_data.csv', parse_dates=[0], dayfirst=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data.iloc[:, 1:])\n",
        "scaled_data = pd.DataFrame(scaled_data, columns=data.columns[1:])\n",
        "scaled_data['Date'] = data.iloc[:, 0]\n",
        "scaled_data.set_index('Date', inplace=True)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data.iloc[i:i+seq_length, :-1].values)\n",
        "        y.append(data.iloc[i+seq_length, -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for CNN\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
        "\n",
        "# Define CNN model\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_cnn.compile(optimizer='adam', loss='mse')\n",
        "model_cnn.summary()\n",
        "\n",
        "# Train the CNN model\n",
        "model_cnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate and predict with the CNN model\n",
        "loss = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss (MSE): {loss}')\n",
        "y_pred = model_cnn.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f'RMSE: {rmse(y_test, y_pred)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palFJ7Ynve1d",
        "outputId": "d59ccb01-b41a-449b-cc0d-1fe3feb5e457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficient of Correlation: 0.9221579063898298\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import pearsonr\n",
        "# Calculate coefficient of correlation\n",
        "corr, _ = pearsonr(y_test_orig, y_pred)\n",
        "print(f'Coefficient of Correlation: {corr}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAwGp01Et6RF",
        "outputId": "0871f8dd-695b-44fd-b087-1868a50072b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_9 (Flatten)         (None, 90)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 64)                5824      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5889 (23.00 KB)\n",
            "Trainable params: 5889 (23.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "307/307 [==============================] - 2s 4ms/step - loss: 0.0538 - val_loss: 0.0084\n",
            "Epoch 2/10\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 0.0080 - val_loss: 0.0063\n",
            "Epoch 3/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 4/10\n",
            "307/307 [==============================] - 1s 4ms/step - loss: 0.0061 - val_loss: 0.0052\n",
            "Epoch 5/10\n",
            "307/307 [==============================] - 1s 3ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 6/10\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 0.0055 - val_loss: 0.0061\n",
            "Epoch 7/10\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 0.0053 - val_loss: 0.0050\n",
            "Epoch 8/10\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 0.0052 - val_loss: 0.0051\n",
            "Epoch 9/10\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 10/10\n",
            "307/307 [==============================] - 1s 2ms/step - loss: 0.0049 - val_loss: 0.0044\n",
            "Test Loss (MSE): 0.004698020871728659\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "RMSE: 2.2395535985895783\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv('/content/merged_file_data.csv', parse_dates=[0], dayfirst=True)\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data.iloc[:, 1:])\n",
        "scaled_data = pd.DataFrame(scaled_data, columns=data.columns[1:])\n",
        "scaled_data['Date'] = data.iloc[:, 0]\n",
        "scaled_data.set_index('Date', inplace=True)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data.iloc[i:i+seq_length, :-1].values)\n",
        "        y.append(data.iloc[i+seq_length, -1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define MLP model\n",
        "model_mlp = Sequential([\n",
        "    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_mlp.compile(optimizer='adam', loss='mse')\n",
        "model_mlp.summary()\n",
        "\n",
        "# Train the MLP model\n",
        "model_mlp.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate and predict with the MLP model\n",
        "loss = model_mlp.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss (MSE): {loss}')\n",
        "y_pred = model_mlp.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f'RMSE: {rmse(y_test, y_pred)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w02pZ6vvSWz",
        "outputId": "73cd7895-5a67-420a-b96c-e41ac9549f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss (MSE): 1030.578857421875\n",
            "96/96 [==============================] - 0s 2ms/step\n",
            "RMSE: 1048.9254600569407\n",
            "Coefficient of Correlation: 0.9221579063898298\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Evaluate and predict with the MLP model\n",
        "loss = model_mlp.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test Loss (MSE): {loss}')\n",
        "y_pred = model_mlp.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "y_test_orig = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "# Calculate RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "rmse_value = rmse(y_test_orig, y_pred)\n",
        "print(f'RMSE: {rmse_value}')\n",
        "\n",
        "# Calculate coefficient of correlation\n",
        "corr, _ = pearsonr(y_test_orig, y_pred)\n",
        "print(f'Coefficient of Correlation: {corr}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uYdQN-VPIkF"
      },
      "outputs": [],
      "source": [
        "# Define LSTM model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1)\n",
        "])\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define CNN model\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_cnn.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define MLP model\n",
        "model_mlp = Sequential([\n",
        "    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_mlp.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMMk3zGIreY8",
        "outputId": "223e6a36-de8d-447f-82c8-513f5f24a763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "384/384 [==============================] - 5s 7ms/step - loss: 0.0130\n",
            "Epoch 2/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0053\n",
            "Epoch 3/50\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0049\n",
            "Epoch 4/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0046\n",
            "Epoch 5/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0044\n",
            "Epoch 6/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0043\n",
            "Epoch 7/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0042\n",
            "Epoch 8/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0042\n",
            "Epoch 9/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0041\n",
            "Epoch 10/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0041\n",
            "Epoch 11/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0040\n",
            "Epoch 12/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0040\n",
            "Epoch 13/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0039\n",
            "Epoch 14/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0039\n",
            "Epoch 15/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0039\n",
            "Epoch 16/50\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0038\n",
            "Epoch 17/50\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0038\n",
            "Epoch 18/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0038\n",
            "Epoch 19/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0038\n",
            "Epoch 20/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0038\n",
            "Epoch 21/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0037\n",
            "Epoch 22/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0037\n",
            "Epoch 23/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0037\n",
            "Epoch 24/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0037\n",
            "Epoch 25/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0037\n",
            "Epoch 26/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 27/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0036\n",
            "Epoch 28/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 29/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 30/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 31/50\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0035\n",
            "Epoch 32/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0035\n",
            "Epoch 33/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0035\n",
            "Epoch 34/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0035\n",
            "Epoch 35/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0035\n",
            "Epoch 36/50\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0035\n",
            "Epoch 37/50\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0034\n",
            "Epoch 38/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0035\n",
            "Epoch 39/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0034\n",
            "Epoch 40/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0034\n",
            "Epoch 41/50\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0034\n",
            "Epoch 42/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0034\n",
            "Epoch 43/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 44/50\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0033\n",
            "Epoch 45/50\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0033\n",
            "Epoch 46/50\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0034\n",
            "Epoch 47/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 48/50\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 49/50\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0033\n",
            "Epoch 50/50\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0033\n",
            "96/96 [==============================] - 1s 4ms/step\n",
            "Epoch 1/50\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0120\n",
            "Epoch 2/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0057\n",
            "Epoch 3/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0051\n",
            "Epoch 4/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0049\n",
            "Epoch 5/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0044\n",
            "Epoch 6/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0044\n",
            "Epoch 7/50\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0043\n",
            "Epoch 8/50\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0041\n",
            "Epoch 9/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0042\n",
            "Epoch 10/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0040\n",
            "Epoch 11/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0039\n",
            "Epoch 12/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0040\n",
            "Epoch 13/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0039\n",
            "Epoch 14/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0038\n",
            "Epoch 15/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0039\n",
            "Epoch 16/50\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0037\n",
            "Epoch 17/50\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0038\n",
            "Epoch 18/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 19/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0037\n",
            "Epoch 20/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0036\n",
            "Epoch 21/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 22/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 23/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 24/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 25/50\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0036\n",
            "Epoch 26/50\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0035\n",
            "Epoch 27/50\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0035\n",
            "Epoch 28/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0034\n",
            "Epoch 29/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0035\n",
            "Epoch 30/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0034\n",
            "Epoch 31/50\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0034\n",
            "Epoch 32/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 33/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 34/50\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0034\n",
            "Epoch 35/50\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0034\n",
            "Epoch 36/50\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0033\n",
            "Epoch 37/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 38/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 39/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 40/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 41/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 42/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 43/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 44/50\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0032\n",
            "Epoch 45/50\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0032\n",
            "Epoch 46/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 47/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 48/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 49/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 50/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "96/96 [==============================] - 0s 2ms/step\n",
            "Epoch 1/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0106\n",
            "Epoch 2/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Epoch 3/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Epoch 4/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Epoch 5/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 6/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0049\n",
            "Epoch 7/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0046\n",
            "Epoch 8/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0045\n",
            "Epoch 9/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0046\n",
            "Epoch 10/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 11/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 12/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Epoch 13/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Epoch 14/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Epoch 15/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Epoch 16/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Epoch 17/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Epoch 18/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Epoch 19/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 20/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 21/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 22/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 23/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0039\n",
            "Epoch 24/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 25/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 26/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 27/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 28/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 29/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 30/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 31/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 32/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 33/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 34/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 35/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 36/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 37/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 38/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 39/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 40/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 41/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 42/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 43/50\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 44/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 45/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 46/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 47/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 48/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 49/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 50/50\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Model: LSTM, RMSE: 32.1091488620828\n",
            "Model: CNN, RMSE: 32.10019106152064\n",
            "Model: MLP, RMSE: 32.104052152315624\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have already loaded and preprocessed your data\n",
        "# X_train, X_test, y_train, y_test should be defined\n",
        "# If scaling is needed, it should have been done before this step\n",
        "\n",
        "# Initialize lists to store the results\n",
        "rmse_per_model = {'LSTM': [], 'CNN': [], 'MLP': []}\n",
        "\n",
        "# Define a function to calculate RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Scale the data if needed (example with MinMaxScaler)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "# Ensure y_train and y_test are reshaped properly if needed\n",
        "# y_train = y_train.reshape(-1, 1)\n",
        "# y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Define and compile the LSTM model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1)\n",
        "])\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define and compile the CNN model\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_cnn.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define and compile the MLP model\n",
        "model_mlp = Sequential([\n",
        "    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_mlp.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Dictionary to store models\n",
        "models = {'LSTM': model_lstm, 'CNN': model_cnn, 'MLP': model_mlp}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Ensure input shape is correct for each model\n",
        "    if model_name == 'CNN':\n",
        "        X_train_input = X_train_scaled.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "        X_test_input = X_test_scaled.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
        "    else:\n",
        "        X_train_input = X_train_scaled\n",
        "        X_test_input = X_test_scaled\n",
        "\n",
        "    model.fit(X_train_input, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "    y_pred = model.predict(X_test_input)\n",
        "\n",
        "    # Inverse transform predictions if needed\n",
        "    y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], X_test.shape[2] - 1)), y_pred), axis=1))[:, -1]\n",
        "    y_test_orig = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], X_test.shape[2] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "    # Calculate and store RMSE\n",
        "    rmse_value = rmse(y_test_orig, y_pred)\n",
        "    rmse_per_model[model_name].append(rmse_value)\n",
        "\n",
        "# Print the RMSE for each model\n",
        "for model_name, rmse_values in rmse_per_model.items():\n",
        "    print(f'Model: {model_name}, RMSE: {np.mean(rmse_values)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gjvzCjV2rnZ",
        "outputId": "6e6e07de-15ac-4737-d7c8-b25ade8c42d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "384/384 [==============================] - 7s 9ms/step - loss: 0.0108\n",
            "Epoch 2/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0052\n",
            "Epoch 3/100\n",
            "384/384 [==============================] - 5s 13ms/step - loss: 0.0047\n",
            "Epoch 4/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0045\n",
            "Epoch 5/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0044\n",
            "Epoch 6/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0042\n",
            "Epoch 7/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0042\n",
            "Epoch 8/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0041\n",
            "Epoch 9/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0041\n",
            "Epoch 10/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0040\n",
            "Epoch 11/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0040\n",
            "Epoch 12/100\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0039\n",
            "Epoch 13/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0039\n",
            "Epoch 14/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0039\n",
            "Epoch 15/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0039\n",
            "Epoch 16/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0039\n",
            "Epoch 17/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0039\n",
            "Epoch 18/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0038\n",
            "Epoch 19/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0038\n",
            "Epoch 20/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0038\n",
            "Epoch 21/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0037\n",
            "Epoch 22/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0037\n",
            "Epoch 23/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0037\n",
            "Epoch 24/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0037\n",
            "Epoch 25/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0037\n",
            "Epoch 26/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0037\n",
            "Epoch 27/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 28/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 29/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0036\n",
            "Epoch 30/100\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0036\n",
            "Epoch 31/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0036\n",
            "Epoch 32/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0035\n",
            "Epoch 33/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0035\n",
            "Epoch 34/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0035\n",
            "Epoch 35/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0035\n",
            "Epoch 36/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0034\n",
            "Epoch 37/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0034\n",
            "Epoch 38/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0034\n",
            "Epoch 39/100\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0034\n",
            "Epoch 40/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0034\n",
            "Epoch 41/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 42/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 43/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0033\n",
            "Epoch 44/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0034\n",
            "Epoch 45/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 46/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 47/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0032\n",
            "Epoch 48/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0033\n",
            "Epoch 49/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0033\n",
            "Epoch 50/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0032\n",
            "Epoch 51/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0033\n",
            "Epoch 52/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0032\n",
            "Epoch 53/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0032\n",
            "Epoch 54/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0031\n",
            "Epoch 55/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0031\n",
            "Epoch 56/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0031\n",
            "Epoch 57/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0031\n",
            "Epoch 58/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0031\n",
            "Epoch 59/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0031\n",
            "Epoch 60/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0031\n",
            "Epoch 61/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0030\n",
            "Epoch 62/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0031\n",
            "Epoch 63/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0030\n",
            "Epoch 64/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0030\n",
            "Epoch 65/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0030\n",
            "Epoch 66/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0030\n",
            "Epoch 67/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0030\n",
            "Epoch 68/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0030\n",
            "Epoch 69/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0029\n",
            "Epoch 70/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0029\n",
            "Epoch 71/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0029\n",
            "Epoch 72/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0029\n",
            "Epoch 73/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0029\n",
            "Epoch 74/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0029\n",
            "Epoch 75/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0029\n",
            "Epoch 76/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0029\n",
            "Epoch 77/100\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0029\n",
            "Epoch 78/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0028\n",
            "Epoch 79/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0028\n",
            "Epoch 80/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0028\n",
            "Epoch 81/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0028\n",
            "Epoch 82/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0028\n",
            "Epoch 83/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0028\n",
            "Epoch 84/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0028\n",
            "Epoch 85/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0028\n",
            "Epoch 86/100\n",
            "384/384 [==============================] - 4s 11ms/step - loss: 0.0028\n",
            "Epoch 87/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0027\n",
            "Epoch 88/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0027\n",
            "Epoch 89/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0027\n",
            "Epoch 90/100\n",
            "384/384 [==============================] - 4s 9ms/step - loss: 0.0027\n",
            "Epoch 91/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0027\n",
            "Epoch 92/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0027\n",
            "Epoch 93/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0027\n",
            "Epoch 94/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0027\n",
            "Epoch 95/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0026\n",
            "Epoch 96/100\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0026\n",
            "Epoch 97/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0026\n",
            "Epoch 98/100\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0026\n",
            "Epoch 99/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0026\n",
            "Epoch 100/100\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0026\n",
            "96/96 [==============================] - 1s 5ms/step\n",
            "Epoch 1/100\n",
            "384/384 [==============================] - 2s 3ms/step - loss: 0.0153\n",
            "Epoch 2/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0057\n",
            "Epoch 3/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0051\n",
            "Epoch 4/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0048\n",
            "Epoch 5/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0047\n",
            "Epoch 6/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0046\n",
            "Epoch 7/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0044\n",
            "Epoch 8/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0042\n",
            "Epoch 9/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0042\n",
            "Epoch 10/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0042\n",
            "Epoch 11/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0040\n",
            "Epoch 12/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0039\n",
            "Epoch 13/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0040\n",
            "Epoch 14/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0039\n",
            "Epoch 15/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0038\n",
            "Epoch 16/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0038\n",
            "Epoch 17/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0038\n",
            "Epoch 18/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0038\n",
            "Epoch 19/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0037\n",
            "Epoch 20/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 21/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0036\n",
            "Epoch 22/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0036\n",
            "Epoch 23/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 24/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0036\n",
            "Epoch 25/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0035\n",
            "Epoch 26/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0035\n",
            "Epoch 27/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0035\n",
            "Epoch 28/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 29/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 30/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0035\n",
            "Epoch 31/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0034\n",
            "Epoch 32/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 33/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 34/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0033\n",
            "Epoch 35/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0033\n",
            "Epoch 36/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 37/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 38/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0033\n",
            "Epoch 39/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 40/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 41/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0033\n",
            "Epoch 42/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 43/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0032\n",
            "Epoch 44/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0032\n",
            "Epoch 45/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 46/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 47/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 48/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 49/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 50/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0031\n",
            "Epoch 51/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0031\n",
            "Epoch 52/100\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0031\n",
            "Epoch 53/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0032\n",
            "Epoch 54/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0031\n",
            "Epoch 55/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 56/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 57/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0030\n",
            "Epoch 58/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 59/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 60/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 61/100\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0030\n",
            "Epoch 62/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0030\n",
            "Epoch 63/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0030\n",
            "Epoch 64/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 65/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 66/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 67/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 68/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0030\n",
            "Epoch 69/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0030\n",
            "Epoch 70/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 71/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0029\n",
            "Epoch 72/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0029\n",
            "Epoch 73/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0029\n",
            "Epoch 74/100\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0029\n",
            "Epoch 75/100\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0029\n",
            "Epoch 76/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0029\n",
            "Epoch 77/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0029\n",
            "Epoch 78/100\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0029\n",
            "Epoch 79/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0029\n",
            "Epoch 80/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0028\n",
            "Epoch 81/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 82/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 83/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 84/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 85/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 86/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 87/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 88/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0027\n",
            "Epoch 89/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0028\n",
            "Epoch 90/100\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0027\n",
            "Epoch 91/100\n",
            "384/384 [==============================] - 1s 4ms/step - loss: 0.0027\n",
            "Epoch 92/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 93/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 94/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 95/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 96/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 97/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 98/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0027\n",
            "Epoch 99/100\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0026\n",
            "Epoch 100/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "96/96 [==============================] - 0s 2ms/step\n",
            "Epoch 1/100\n",
            "384/384 [==============================] - 2s 2ms/step - loss: 0.0121\n",
            "Epoch 2/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0064\n",
            "Epoch 3/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Epoch 4/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Epoch 5/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Epoch 6/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Epoch 7/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Epoch 8/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Epoch 9/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Epoch 10/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 11/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 12/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0043\n",
            "Epoch 13/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Epoch 14/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Epoch 15/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Epoch 16/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 17/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Epoch 18/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 19/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 20/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 21/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Epoch 22/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 23/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 24/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 25/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Epoch 26/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 27/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 28/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 29/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 30/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 31/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 32/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 33/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 34/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 35/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 36/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 37/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Epoch 38/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Epoch 39/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 40/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 41/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 42/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 43/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 44/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 45/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 46/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 47/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 48/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Epoch 49/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 50/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Epoch 51/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 52/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 53/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 54/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 55/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 56/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 57/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 58/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 59/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 60/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 61/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Epoch 62/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 63/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 64/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 65/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 66/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 67/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 68/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 69/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 70/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 71/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 72/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 73/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 74/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 75/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 76/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 77/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 78/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 79/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 80/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 81/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 82/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 83/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 84/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 85/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 86/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 87/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 88/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 89/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 90/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 91/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Epoch 92/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 93/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 94/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 95/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 96/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 97/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 98/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Epoch 99/100\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 100/100\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "96/96 [==============================] - 0s 2ms/step\n",
            "Model: LSTM, RMSE: 32.106630404558715, Correlation Coefficient: 0.9521068785842279\n",
            "Model: CNN, RMSE: 32.12125760428836, Correlation Coefficient: 0.9484466678249808\n",
            "Model: MLP, RMSE: 32.1117816098898, Correlation Coefficient: 0.9465270230845775\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have already loaded and preprocessed your data\n",
        "# X_train, X_test, y_train, y_test should be defined\n",
        "# If scaling is needed, it should have been done before this step\n",
        "\n",
        "# Initialize lists and dictionaries to store the results\n",
        "rmse_per_model = {'LSTM': [], 'CNN': [], 'MLP': []}\n",
        "corr_coef_per_model = {'LSTM': [], 'CNN': [], 'MLP': []}\n",
        "\n",
        "# Define a function to calculate RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Scale the data if needed (example with MinMaxScaler)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "# Ensure y_train and y_test are reshaped properly if needed\n",
        "# y_train = y_train.reshape(-1, 1)\n",
        "# y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Define and compile the LSTM model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1)\n",
        "])\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define and compile the CNN model\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_cnn.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Define and compile the MLP model\n",
        "model_mlp = Sequential([\n",
        "    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_mlp.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Dictionary to store models\n",
        "models = {'LSTM': model_lstm, 'CNN': model_cnn, 'MLP': model_mlp}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Ensure input shape is correct for each model\n",
        "    if model_name == 'CNN':\n",
        "        X_train_input = X_train_scaled.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "        X_test_input = X_test_scaled.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
        "    else:\n",
        "        X_train_input = X_train_scaled\n",
        "        X_test_input = X_test_scaled\n",
        "\n",
        "    model.fit(X_train_input, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "    y_pred = model.predict(X_test_input)\n",
        "\n",
        "    # Inverse transform predictions if needed\n",
        "    y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], X_test.shape[2] - 1)), y_pred), axis=1))[:, -1]\n",
        "    y_test_orig = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], X_test.shape[2] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "    # Calculate and store RMSE\n",
        "    rmse_value = rmse(y_test_orig, y_pred)\n",
        "    rmse_per_model[model_name].append(rmse_value)\n",
        "\n",
        "    # Calculate and store coefficient of correlation\n",
        "    corr_coef = np.corrcoef(y_test_orig, y_pred)[0, 1]\n",
        "    corr_coef_per_model[model_name].append(corr_coef)\n",
        "\n",
        "# Print the RMSE and coefficient of correlation for each model\n",
        "for model_name, rmse_values in rmse_per_model.items():\n",
        "    print(f'Model: {model_name}, RMSE: {np.mean(rmse_values)}, Correlation Coefficient: {np.mean(corr_coef_per_model[model_name])}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrC6TA1UOd26"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the results\n",
        "rmse_per_model = {'LSTM': [], 'CNN': [], 'MLP': []}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D1f8yc1jsOx",
        "outputId": "1f8044e7-13a2-4498-f18a-7d3751050eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "384/384 [==============================] - 3s 8ms/step - loss: 0.0032\n",
            "Epoch 2/5\n",
            "384/384 [==============================] - 4s 10ms/step - loss: 0.0033\n",
            "Epoch 3/5\n",
            "384/384 [==============================] - 2s 6ms/step - loss: 0.0032\n",
            "Epoch 4/5\n",
            "384/384 [==============================] - 3s 9ms/step - loss: 0.0032\n",
            "Epoch 5/5\n",
            "384/384 [==============================] - 3s 7ms/step - loss: 0.0032\n",
            "96/96 [==============================] - 0s 3ms/step\n",
            "Epoch 1/5\n",
            "384/384 [==============================] - 2s 4ms/step - loss: 0.0033\n",
            "Epoch 2/5\n",
            "384/384 [==============================] - 2s 5ms/step - loss: 0.0032\n",
            "Epoch 3/5\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 4/5\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 5/5\n",
            "384/384 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "96/96 [==============================] - 0s 2ms/step\n",
            "Epoch 1/5\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Epoch 2/5\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Epoch 3/5\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Epoch 4/5\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Epoch 5/5\n",
            "384/384 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "96/96 [==============================] - 0s 1ms/step\n",
            "Model: LSTM, RMSE: 5392554.804033854\n",
            "Model: CNN, RMSE: 176197347.85489184\n",
            "Model: MLP, RMSE: 13433223480.895035\n"
          ]
        }
      ],
      "source": [
        "# prompt: print rmse of per model\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in {'LSTM': model_lstm, 'CNN': model_cnn, 'MLP': model_mlp}.items():\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = scaler.inverse_transform(np.concatenate((np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred), axis=1))[:, -1]\n",
        "    y_test = scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "    rmse_per_model[model_name].append(rmse(y_test, y_pred))\n",
        "\n",
        "# Print the RMSE for each model\n",
        "for model_name, rmse_values in rmse_per_model.items():\n",
        "    print(f'Model: {model_name}, RMSE: {np.mean(rmse_values)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcA0euOeQI1S",
        "outputId": "f2e58bbd-5486-4e01-a122-2821efd50233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 1s 6ms/step\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "77/77 [==============================] - 0s 2ms/step\n",
            "77/77 [==============================] - 0s 3ms/step\n",
            "77/77 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Iterate through each fold\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    # Get the training and testing data for the current fold\n",
        "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # Reshape the data if necessary for each model\n",
        "    X_train_fold_lstm = X_train_fold.reshape((X_train_fold.shape[0], X_train_fold.shape[1], X_train_fold.shape[2], 1))\n",
        "    X_test_fold_lstm = X_test_fold.reshape((X_test_fold.shape[0], X_test_fold.shape[1], X_test_fold.shape[2], 1))\n",
        "\n",
        "    # Train and evaluate LSTM model\n",
        "    model_lstm.fit(X_train_fold_lstm, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
        "    y_pred_lstm = model_lstm.predict(X_test_fold_lstm)\n",
        "    rmse_lstm = np.sqrt(mean_squared_error(y_test_fold, y_pred_lstm))\n",
        "    rmse_per_model['LSTM'].append(rmse_lstm)\n",
        "\n",
        "    # Train and evaluate CNN model\n",
        "    model_cnn.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
        "    y_pred_cnn = model_cnn.predict(X_test_fold)\n",
        "    rmse_cnn = np.sqrt(mean_squared_error(y_test_fold, y_pred_cnn))\n",
        "    rmse_per_model['CNN'].append(rmse_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DIcwCkVOr9P"
      },
      "outputs": [],
      "source": [
        "# Define MLP model\n",
        "model_mlp = Sequential([\n",
        "    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model_mlp.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXdkbzIqlABa",
        "outputId": "97d5f37f-0601-42ce-9eb3-ee259c02e064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate MLP model\n",
        "model_mlp.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
        "y_pred_mlp = model_mlp.predict(X_test_fold)\n",
        "rmse_mlp = np.sqrt(mean_squared_error(y_test_fold, y_pred_mlp))\n",
        "rmse_per_model['MLP'].append(rmse_mlp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkFUqWzWlFlO",
        "outputId": "d0dabcff-9a1e-4776-aa9c-97b34eacb912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: LSTM, RMSE: 174.87824344036292\n",
            "Model: CNN, RMSE: 5887.361146342601\n",
            "Model: MLP, RMSE: 577609.9253042161\n"
          ]
        }
      ],
      "source": [
        "# prompt: calculate the rmse for above lstm,cnn and mlp model\n",
        "\n",
        "# Calculate the RMSE for each model\n",
        "for model_name, rmse_values in rmse_per_model.items():\n",
        "    print(f'Model: {model_name}, RMSE: {np.mean(rmse_values)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAOS2yGRUyj",
        "outputId": "d8901a9a-67f6-4b4a-be14-cb4d99bc747e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 0s 2ms/step\n",
            "Average RMSE for LSTM: 0.06097418912534851\n",
            "Average RMSE for CNN: 0.059910658735339226\n",
            "Average RMSE for MLP: 0.06692094587191981\n"
          ]
        }
      ],
      "source": [
        "# prompt: add code of finding rmse value of all this model(lstm ,cnn,mlp)\n",
        "\n",
        "# Train and evaluate MLP model\n",
        "model_mlp.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
        "y_pred_mlp = model_mlp.predict(X_test_fold)\n",
        "rmse_mlp = np.sqrt(mean_squared_error(y_test_fold, y_pred_mlp))\n",
        "rmse_per_model['MLP'].append(rmse_mlp)\n",
        "\n",
        "# Calculate average RMSE for each model\n",
        "average_rmse = {model: sum(rmse_values) / len(rmse_values) for model, rmse_values in rmse_per_model.items()}\n",
        "\n",
        "# Print average RMSE for each model\n",
        "for model, rmse in average_rmse.items():\n",
        "    print(f\"Average RMSE for {model}: {rmse}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}